# Data Analytics With Hadoop

## Introduction

### The Need for Big Data Tools<sup>1</sup>

Imagine you are the general manager of a football team, and need to make some critical roster decisions: Which players do you resign for the next year? Which players should you cut?

Out of these questions, two strategies emerge:

1. Build your team around one superstar player.

2. Build a team of average, if not mediocre players who are versatile and capable of working together well.

Within the data engineering industry, we've typically picked Strategy 1 (One Superstar): we'll buy a supercomputer, with cutting edge processor speed, memory bandwidth, and other excellent performance metrics. We ask this supercomputer to do everything, all the time.

The issue, however, is that putting all your eggs in one basket typically doesn't work out in the long run, in both sports and data engineering.

Consider the following scenarios:

- **size**: Our star player typically is more physically dominant than his/her opponents. But what happens when he/she competes against someone even more athletic? Our supercomputer typically handles most data requests with ease, but what if the data we want to process is larger than the disk space or memory capacity of the supercomputer
-


## Notes

1. Much of the content from this section is summarized from a Pluralsight online course called [Building Blocks of Hadoop, HDFS, MapReduce, & Yarn by Janani Ravi](https://app.pluralsight.com/player?course=building-blocks-hadoop-hdfs-mapreduce-yarn&author=janani-ravi&name=building-blocks-hadoop-hdfs-mapreduce-yarn-m1&clip=2&mode=live). 

